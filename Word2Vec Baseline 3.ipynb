{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aceaa52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity via Word2Vec Baseline 3:  38.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "directory = r'C:\\Users\\DELL\\Desktop\\Ass3\\Doc50'\n",
    "directoryA = r'C:\\Users\\DELL\\Desktop\\Ass3\\Doc50 GT\\C1'\n",
    "directoryB = r'C:\\Users\\DELL\\Desktop\\Ass3\\Doc50 GT\\C2'\n",
    "directoryC = r'C:\\Users\\DELL\\Desktop\\Ass3\\Doc50 GT\\C3'\n",
    "directoryD = r'C:\\Users\\DELL\\Desktop\\Ass3\\Doc50 GT\\C4'\n",
    "directoryE = r'C:\\Users\\DELL\\Desktop\\Ass3\\Doc50 GT\\C5'\n",
    "file_contents = []\n",
    "name = []\n",
    "remove = \"\"\n",
    "C1 = []\n",
    "C2 = []\n",
    "C3 = []\n",
    "C4 = []\n",
    "C5 = []\n",
    "\n",
    "for filename in os.listdir(directoryA):\n",
    "    C1.append(filename)\n",
    "    \n",
    "for filename in os.listdir(directoryB):\n",
    "    C2.append(filename)\n",
    "    \n",
    "for filename in os.listdir(directoryC):\n",
    "    C3.append(filename)\n",
    "    \n",
    "for filename in os.listdir(directoryD):\n",
    "    C4.append(filename)\n",
    "    \n",
    "for filename in os.listdir(directoryE):\n",
    "    C5.append(filename)\n",
    "\n",
    "def remove_prepositions(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    filtered = [word for word, pos in tagged if pos not in ['IN', 'TO']]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = [\"was\",\"wasn't\",\"was not\",\"donot\",\"don't\",\"there\",\"here\",\"never\",\"n't\",\"s\",\"been\",\"will\",\"would\",\"wouldn't\",\"hasn't\",\"has not\",\"haven't\",\"have not\",\"doesn't\",\"had\",\"hadn't\",\"had not\",\"upto\",\"and\",\"or\",\"not\",\"nt\",\"that\",\"this\",\"it\",\"i\",\"why\",\"who\",\"how\",\"what\",\"when\",\"he\",\"she\",\"their\",\"you\",\"but\",\"because\",\"then\",\"which\",\"him\",\"her\",\"their\",\"where\",\"could\",\"an\"]\n",
    "translator = str.maketrans('', '', string.punctuation.replace('-', ''))\n",
    "\n",
    "with open(\"C:\\\\Users\\\\DELL\\\\Desktop\\\\Ass3\\\\Stopword-List.txt\", \"r\") as f:\n",
    "    contents = f.read()\n",
    "    words = contents.split()\n",
    "\n",
    "    # Store the words in an array\n",
    "    for word in words:\n",
    "        stopwords.append(word)\n",
    "\n",
    "with open(directory1, 'r') as f:\n",
    "    remove = f.read()\n",
    "    \n",
    "# iterate over all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    \n",
    "    # read the contents of the file\n",
    "    with open(filepath, 'r') as f:\n",
    "        contents = f.read()\n",
    "        if filename == \"64831\":\n",
    "            contents = re.sub(r'^X', '', contents, flags=re.MULTILINE)\n",
    "        name.append(filename)\n",
    "        contents = re.sub(r'[^\\w\\s]', '', contents)\n",
    "        contents = re.sub(r\"\\b\\w*[0-9@]+\\w*\\b\", \"\", contents)\n",
    "        contents = nltk.word_tokenize(contents)\n",
    "        contents = [stemmer.stem(word) for word in contents]\n",
    "        contents = ' '.join(contents)\n",
    "        #contents = \" \".join([lemmatizer.lemmatize(word, pos='v') if pos.startswith('V') else lemmatizer.lemmatize(word) for word, pos in nltk.pos_tag(nltk.word_tokenize(contents))])\n",
    "        contents = contents.translate(translator)\n",
    "        contents = contents.lower()\n",
    "        contents = remove_prepositions(contents)\n",
    "        contents = ' '.join(word for word in contents.split() if word not in stopwords)\n",
    "        \n",
    "    # add the contents to the array\n",
    "    file_contents.append(contents)\n",
    "\n",
    "# Step 1: Split the preprocessed text data into individual words\n",
    "sentences = [doc.split() for doc in file_contents]\n",
    "\n",
    "# Step 2: Train a Word2Vec model on the list of sentences\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=3)\n",
    "\n",
    "\n",
    "# Step 3: Get the average word embedding for each document\n",
    "document_embeddings = []\n",
    "for doc in sentences:\n",
    "    doc_embedding = np.mean([model.wv.get_vector(word) for word in doc if word in model.wv.key_to_index], axis=0)\n",
    "    document_embeddings.append(doc_embedding)\n",
    "\n",
    "# Step 4: Use K-means clustering to cluster the documents based on their average word embeddings\n",
    "kmeans = KMeans(n_clusters=5, random_state=42).fit(document_embeddings)\n",
    "\n",
    "predicted_cluster_labels = kmeans.labels_\n",
    "predicted_cluster_labels+=1\n",
    "true_labels = [int(name.split(\"_\")[0]) - 1 for name in name]\n",
    "\n",
    "cluster_true_pairs = list(zip(predicted_cluster_labels, true_labels))\n",
    "cluster_counts = Counter(predicted_cluster_labels)\n",
    "\n",
    "Ground_label = []\n",
    "\n",
    "for x in range(0, len(name)):\n",
    "    if name[x] in C1:\n",
    "        Ground_label.append(1)\n",
    "    elif name[x] in C2:\n",
    "        Ground_label.append(2)\n",
    "    elif name[x] in C3:\n",
    "        Ground_label.append(3)\n",
    "    elif name[x] in C4:\n",
    "        Ground_label.append(4)\n",
    "    else:\n",
    "        Ground_label.append(5)\n",
    "contingency = contingency_matrix(Ground_label, predicted_cluster_labels)\n",
    "purity = np.sum(np.amax(contingency, axis=0)) / np.sum(contingency)\n",
    "print('Purity via Word2Vec Baseline 3: ',purity*100, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f02f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
