{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "86fb9b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity via TF Baseline 1:  56.00000000000001 %\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "directory = r'C:\\Users\\DELL\\Desktop\\Ass3\\Doc50'\n",
    "directoryA = r'C:\\Users\\DELL\\Desktop\\Ass3\\Doc50 GT\\C1'\n",
    "directoryB = r'C:\\Users\\DELL\\Desktop\\Ass3\\Doc50 GT\\C2'\n",
    "directoryC = r'C:\\Users\\DELL\\Desktop\\Ass3\\Doc50 GT\\C3'\n",
    "directoryD = r'C:\\Users\\DELL\\Desktop\\Ass3\\Doc50 GT\\C4'\n",
    "directoryE = r'C:\\Users\\DELL\\Desktop\\Ass3\\Doc50 GT\\C5'\n",
    "file_contents = []\n",
    "name = []\n",
    "remove = \"\"\n",
    "C1 = []\n",
    "C2 = []\n",
    "C3 = []\n",
    "C4 = []\n",
    "C5 = []\n",
    "\n",
    "for filename in os.listdir(directoryA):\n",
    "    C1.append(filename)\n",
    "    \n",
    "for filename in os.listdir(directoryB):\n",
    "    C2.append(filename)\n",
    "    \n",
    "for filename in os.listdir(directoryC):\n",
    "    C3.append(filename)\n",
    "    \n",
    "for filename in os.listdir(directoryD):\n",
    "    C4.append(filename)\n",
    "    \n",
    "for filename in os.listdir(directoryE):\n",
    "    C5.append(filename)\n",
    "\n",
    "def remove_prepositions(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    filtered = [word for word, pos in tagged if pos not in ['IN', 'TO']]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = [\"was\",\"wasn't\",\"was not\",\"donot\",\"don't\",\"there\",\"here\",\"never\",\"n't\",\"s\",\"been\",\"will\",\"would\",\"wouldn't\",\"hasn't\",\"has not\",\"haven't\",\"have not\",\"doesn't\",\"had\",\"hadn't\",\"had not\",\"upto\",\"and\",\"or\",\"not\",\"nt\",\"that\",\"this\",\"it\",\"i\",\"why\",\"who\",\"how\",\"what\",\"when\",\"he\",\"she\",\"their\",\"you\",\"but\",\"because\",\"then\",\"which\",\"him\",\"her\",\"their\",\"where\",\"could\",\"an\"]\n",
    "translator = str.maketrans('', '', string.punctuation.replace('-', ''))\n",
    "\n",
    "with open(\"C:\\\\Users\\\\DELL\\\\Desktop\\\\Ass3\\\\Stopword-List.txt\", \"r\") as f:\n",
    "    contents = f.read()\n",
    "    words = contents.split()\n",
    "\n",
    "    # Store the words in an array\n",
    "    for word in words:\n",
    "        stopwords.append(word)\n",
    "\n",
    "with open(directory1, 'r') as f:\n",
    "    remove = f.read()\n",
    "    \n",
    "# iterate over all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    \n",
    "    # read the contents of the file\n",
    "    with open(filepath, 'r') as f:\n",
    "        contents = f.read()\n",
    "        if filename == \"64831\":\n",
    "            contents = re.sub(r'^X', '', contents, flags=re.MULTILINE)\n",
    "        name.append(filename)\n",
    "        contents = re.sub(r'[^\\w\\s]', '', contents)\n",
    "        contents = re.sub(r\"\\b\\w*[0-9@]+\\w*\\b\", \"\", contents)\n",
    "        contents = nltk.word_tokenize(contents)\n",
    "        contents = [stemmer.stem(word) for word in contents]\n",
    "        contents = ' '.join(contents)\n",
    "        contents = \" \".join([lemmatizer.lemmatize(word, pos='v') if pos.startswith('V') else lemmatizer.lemmatize(word) for word, pos in nltk.pos_tag(nltk.word_tokenize(contents))])\n",
    "        contents = contents.translate(translator)\n",
    "        contents = contents.lower()\n",
    "        contents = remove_prepositions(contents)\n",
    "        contents = ' '.join(word for word in contents.split() if word not in stopwords)\n",
    "        \n",
    "    # add the contents to the array\n",
    "    file_contents.append(contents)\n",
    "vectorizer = TfidfVectorizer(use_idf=False)\n",
    "\n",
    "# transform the preprocessed text into a matrix of TF-IDF features\n",
    "X_tfidf = vectorizer.fit_transform(file_contents)\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=42, n_init=10).fit(X_tfidf)\n",
    "\n",
    "predicted_cluster_labels = kmeans.labels_\n",
    "predicted_cluster_labels+=1\n",
    "true_labels = [int(name.split(\"_\")[0]) - 1 for name in name]\n",
    "\n",
    "cluster_true_pairs = list(zip(predicted_cluster_labels, true_labels))\n",
    "cluster_counts = Counter(predicted_cluster_labels)\n",
    "\n",
    "Ground_label = []\n",
    "\n",
    "for x in range(0, len(name)):\n",
    "    if name[x] in C1:\n",
    "        Ground_label.append(1)\n",
    "    elif name[x] in C2:\n",
    "        Ground_label.append(2)\n",
    "    elif name[x] in C3:\n",
    "        Ground_label.append(3)\n",
    "    elif name[x] in C4:\n",
    "        Ground_label.append(4)\n",
    "    else:\n",
    "        Ground_label.append(5)\n",
    "contingency = contingency_matrix(Ground_label, predicted_cluster_labels)\n",
    "purity = np.sum(np.amax(contingency, axis=0)) / np.sum(contingency)\n",
    "print('Purity via TF Baseline 1: ',purity*100, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c67366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
